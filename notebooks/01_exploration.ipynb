{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Texas GLO Action Plan - Data Exploration\n",
    "\n",
    "This notebook explores the DRGR (Disaster Recovery Grant Reporting) reports from the Texas General Land Office.\n",
    "\n",
    "**Contents:**\n",
    "1. Overview of available data\n",
    "2. PDF extraction testing\n",
    "3. Sample text analysis\n",
    "4. Table extraction examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, str(Path.cwd().parent / 'src'))\n",
    "\n",
    "# Project imports\n",
    "from config import DRGR_REPORTS_DIR, DATA_DIR, DATABASE_PATH\n",
    "from utils import get_all_pdfs, parse_filename, get_category_from_path, init_database\n",
    "from pdf_processor import PDFProcessor\n",
    "\n",
    "print(f\"Reports directory: {DRGR_REPORTS_DIR}\")\n",
    "print(f\"Data directory: {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Overview\n",
    "\n",
    "Let's see what PDF files we have available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all PDF files\n",
    "pdf_files = get_all_pdfs()\n",
    "print(f\"Total PDF files: {len(pdf_files)}\")\n",
    "\n",
    "# Calculate total size\n",
    "total_size = sum(p.stat().st_size for p in pdf_files)\n",
    "print(f\"Total size: {total_size / (1024**2):.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe of all files with metadata\n",
    "file_data = []\n",
    "for pdf in pdf_files:\n",
    "    meta = parse_filename(pdf.name)\n",
    "    category = get_category_from_path(pdf)\n",
    "    file_data.append({\n",
    "        'filename': pdf.name,\n",
    "        'category': category,\n",
    "        'disaster_code': meta.get('disaster_code'),\n",
    "        'year': meta.get('year'),\n",
    "        'quarter': meta.get('quarter'),\n",
    "        'size_mb': pdf.stat().st_size / (1024**2),\n",
    "    })\n",
    "\n",
    "df_files = pd.DataFrame(file_data)\n",
    "df_files.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary by category\n",
    "category_summary = df_files.groupby('category').agg({\n",
    "    'filename': 'count',\n",
    "    'size_mb': 'sum'\n",
    "}).rename(columns={'filename': 'file_count'}).sort_values('file_count', ascending=False)\n",
    "\n",
    "print(\"Files by Category:\")\n",
    "print(category_summary.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# File count by category\n",
    "category_summary['file_count'].plot(kind='barh', ax=axes[0], color='steelblue')\n",
    "axes[0].set_xlabel('Number of Files')\n",
    "axes[0].set_title('PDF Count by Category')\n",
    "\n",
    "# Size by category\n",
    "category_summary['size_mb'].plot(kind='barh', ax=axes[1], color='coral')\n",
    "axes[1].set_xlabel('Size (MB)')\n",
    "axes[1].set_title('Total Size by Category')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timeline of reports\n",
    "yearly = df_files[df_files['year'].notna()].groupby('year').size()\n",
    "print(\"\\nReports by Year:\")\n",
    "print(yearly.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PDF Text Extraction Test\n",
    "\n",
    "Let's test the text extraction on a sample document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "# Pick a sample file (medium-sized recent report)\n",
    "sample_pdf = DRGR_REPORTS_DIR / \"2019_Disasters_ActionPlan\" / \"drgr-2019-disasters-2025-q4.pdf\"\n",
    "\n",
    "if not sample_pdf.exists():\n",
    "    # Fallback to first available PDF\n",
    "    sample_pdf = pdf_files[0]\n",
    "\n",
    "print(f\"Testing with: {sample_pdf.name}\")\n",
    "print(f\"Size: {sample_pdf.stat().st_size / 1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open and extract text\n",
    "doc = fitz.open(sample_pdf)\n",
    "print(f\"Page count: {len(doc)}\")\n",
    "print(f\"PDF metadata: {doc.metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text from first few pages\n",
    "for page_num in range(min(3, len(doc))):\n",
    "    page = doc[page_num]\n",
    "    text = page.get_text()\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"PAGE {page_num + 1} (first 1000 chars):\")\n",
    "    print('='*60)\n",
    "    print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Table Extraction Test\n",
    "\n",
    "Let's test table extraction using pdfplumber."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "\n",
    "# Use an expenditure report which should have clear tables\n",
    "table_pdf = DRGR_REPORTS_DIR / \"Expenditure_Reports\" / \"cdbg-dr-and-mit-timely-expenditure-report-2020-4q.pdf\"\n",
    "\n",
    "if not table_pdf.exists():\n",
    "    table_pdf = sample_pdf\n",
    "\n",
    "print(f\"Testing tables with: {table_pdf.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract tables\n",
    "with pdfplumber.open(table_pdf) as pdf:\n",
    "    print(f\"Page count: {len(pdf.pages)}\")\n",
    "    \n",
    "    for page_num, page in enumerate(pdf.pages[:5], start=1):\n",
    "        tables = page.extract_tables()\n",
    "        print(f\"\\nPage {page_num}: Found {len(tables)} table(s)\")\n",
    "        \n",
    "        for i, table in enumerate(tables):\n",
    "            if table and len(table) > 0:\n",
    "                df = pd.DataFrame(table[1:], columns=table[0] if table[0] else None)\n",
    "                print(f\"\\n  Table {i+1}: {len(df)} rows x {len(df.columns)} columns\")\n",
    "                print(df.head().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Full Processing\n",
    "\n",
    "Use the PDFProcessor to extract all documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize processor\n",
    "processor = PDFProcessor()\n",
    "\n",
    "# Check current stats\n",
    "stats = processor.get_document_stats()\n",
    "print(\"Current Processing Status:\")\n",
    "print(f\"  Total registered: {stats['total_documents']}\")\n",
    "print(f\"  Processed: {stats['processed_documents']}\")\n",
    "print(f\"  Total pages: {stats['total_pages']}\")\n",
    "print(f\"  Total tables: {stats['total_tables']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process a small batch to test (change limit or remove to process all)\n",
    "# WARNING: Processing all 442 PDFs may take 30-60 minutes\n",
    "\n",
    "# Uncomment to process:\n",
    "# processor.process_all(limit=10)  # Test with 10 files first\n",
    "# processor.process_all()  # Process all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View stats by category\n",
    "stats = processor.get_document_stats()\n",
    "if stats['by_category']:\n",
    "    df_stats = pd.DataFrame(stats['by_category'])\n",
    "    print(\"\\nProcessed by Category:\")\n",
    "    print(df_stats.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Sample Extracted Content\n",
    "\n",
    "Once processing is complete, explore the extracted content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Connect to database\n",
    "conn = sqlite3.connect(DATABASE_PATH)\n",
    "\n",
    "# Query sample documents\n",
    "df_docs = pd.read_sql_query('''\n",
    "    SELECT filename, category, year, quarter, page_count, text_extracted\n",
    "    FROM documents\n",
    "    ORDER BY year DESC, quarter DESC\n",
    "    LIMIT 20\n",
    "''', conn)\n",
    "\n",
    "print(\"Recent Documents:\")\n",
    "df_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample extracted text\n",
    "sample_text = pd.read_sql_query('''\n",
    "    SELECT d.filename, t.page_number, t.char_count, \n",
    "           SUBSTR(t.text_content, 1, 500) as text_preview\n",
    "    FROM document_text t\n",
    "    JOIN documents d ON t.document_id = d.id\n",
    "    WHERE t.char_count > 100\n",
    "    LIMIT 5\n",
    "''', conn)\n",
    "\n",
    "for _, row in sample_text.iterrows():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{row['filename']} - Page {row['page_number']} ({row['char_count']} chars)\")\n",
    "    print('='*60)\n",
    "    print(row['text_preview'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "After running the PDF extraction:\n",
    "\n",
    "1. **Phase 2**: Run NLP entity extraction (see `02_entity_analysis.ipynb`)\n",
    "2. **Phase 3**: Analyze financial tables (see `03_financial_analysis.ipynb`)\n",
    "3. **Phase 4**: Set up semantic search with embeddings\n",
    "4. **Phase 5**: Build interactive dashboard"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
