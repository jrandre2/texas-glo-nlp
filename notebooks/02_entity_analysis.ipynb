{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Texas GLO - Entity Analysis\n",
    "\n",
    "This notebook explores the entities extracted from the DRGR disaster recovery reports using NLP.\n",
    "\n",
    "**Contents:**\n",
    "1. Entity extraction statistics\n",
    "2. Analysis by entity type\n",
    "3. Geographic entity analysis\n",
    "4. Financial entity analysis\n",
    "5. Disaster-specific entity analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sqlite3\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, str(Path.cwd().parent / 'src'))\n",
    "\n",
    "from config import DATABASE_PATH, EXPORTS_DIR\n",
    "from nlp_processor import NLPProcessor\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(f\"Database: {DATABASE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Entity Extraction Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to database\n",
    "conn = sqlite3.connect(DATABASE_PATH)\n",
    "\n",
    "# Get overall stats\n",
    "stats = pd.read_sql_query('''\n",
    "    SELECT \n",
    "        COUNT(*) as total_entities,\n",
    "        COUNT(DISTINCT document_id) as documents_with_entities,\n",
    "        COUNT(DISTINCT entity_type) as entity_types,\n",
    "        COUNT(DISTINCT entity_text) as unique_values\n",
    "    FROM entities\n",
    "''', conn)\n",
    "\n",
    "print(\"Entity Extraction Summary:\")\n",
    "print(f\"  Total entities: {stats['total_entities'].iloc[0]:,}\")\n",
    "print(f\"  Documents with entities: {stats['documents_with_entities'].iloc[0]}\")\n",
    "print(f\"  Entity types: {stats['entity_types'].iloc[0]}\")\n",
    "print(f\"  Unique entity values: {stats['unique_values'].iloc[0]:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entities by type\n",
    "df_types = pd.read_sql_query('''\n",
    "    SELECT entity_type, COUNT(*) as count, COUNT(DISTINCT entity_text) as unique_values\n",
    "    FROM entities\n",
    "    GROUP BY entity_type\n",
    "    ORDER BY count DESC\n",
    "''', conn)\n",
    "\n",
    "print(\"\\nEntities by Type:\")\n",
    "df_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize entity distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Top 15 entity types by count\n",
    "df_top = df_types.head(15)\n",
    "axes[0].barh(df_top['entity_type'], df_top['count'], color='steelblue')\n",
    "axes[0].set_xlabel('Count')\n",
    "axes[0].set_title('Top 15 Entity Types by Count')\n",
    "axes[0].invert_yaxis()\n",
    "\n",
    "# Unique values per type\n",
    "axes[1].barh(df_top['entity_type'], df_top['unique_values'], color='coral')\n",
    "axes[1].set_xlabel('Unique Values')\n",
    "axes[1].set_title('Unique Values per Entity Type')\n",
    "axes[1].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Domain-Specific Entities\n",
    "\n",
    "Let's examine the custom disaster recovery entities we extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disaster entities\n",
    "df_disasters = pd.read_sql_query('''\n",
    "    SELECT entity_text, COUNT(*) as mentions\n",
    "    FROM entities\n",
    "    WHERE entity_type = 'DISASTER'\n",
    "    GROUP BY entity_text\n",
    "    ORDER BY mentions DESC\n",
    "    LIMIT 20\n",
    "''', conn)\n",
    "\n",
    "print(\"Top Disaster Mentions:\")\n",
    "df_disasters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEMA declarations\n",
    "df_fema = pd.read_sql_query('''\n",
    "    SELECT entity_text, COUNT(*) as mentions\n",
    "    FROM entities\n",
    "    WHERE entity_type = 'FEMA_DECLARATION'\n",
    "    GROUP BY entity_text\n",
    "    ORDER BY mentions DESC\n",
    "    LIMIT 20\n",
    "''', conn)\n",
    "\n",
    "print(\"FEMA Declarations Found:\")\n",
    "df_fema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Program names\n",
    "df_programs = pd.read_sql_query('''\n",
    "    SELECT entity_text, COUNT(*) as mentions\n",
    "    FROM entities\n",
    "    WHERE entity_type = 'PROGRAM'\n",
    "    GROUP BY entity_text\n",
    "    ORDER BY mentions DESC\n",
    "    LIMIT 20\n",
    "''', conn)\n",
    "\n",
    "print(\"Recovery Programs Mentioned:\")\n",
    "df_programs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Geographic Analysis\n",
    "\n",
    "Analyze Texas counties and locations mentioned in the reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Texas counties\n",
    "df_counties = pd.read_sql_query('''\n",
    "    SELECT entity_text, COUNT(*) as mentions\n",
    "    FROM entities\n",
    "    WHERE entity_type = 'TX_COUNTY'\n",
    "    GROUP BY entity_text\n",
    "    ORDER BY mentions DESC\n",
    "    LIMIT 30\n",
    "''', conn)\n",
    "\n",
    "print(\"Most Mentioned Texas Counties:\")\n",
    "df_counties.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top counties\n",
    "if len(df_counties) > 0:\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    top_counties = df_counties.head(20)\n",
    "    ax.barh(top_counties['entity_text'], top_counties['mentions'], color='teal')\n",
    "    ax.set_xlabel('Number of Mentions')\n",
    "    ax.set_title('Top 20 Texas Counties in Disaster Reports')\n",
    "    ax.invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All geographic entities (GPE = Geopolitical Entity)\n",
    "df_gpe = pd.read_sql_query('''\n",
    "    SELECT entity_text, COUNT(*) as mentions\n",
    "    FROM entities\n",
    "    WHERE entity_type = 'GPE'\n",
    "    GROUP BY entity_text\n",
    "    ORDER BY mentions DESC\n",
    "    LIMIT 30\n",
    "''', conn)\n",
    "\n",
    "print(\"Top Geographic Entities (GPE):\")\n",
    "df_gpe.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Financial Analysis\n",
    "\n",
    "Analyze money amounts mentioned in the reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top money amounts\n",
    "df_money = pd.read_sql_query('''\n",
    "    SELECT entity_text, COUNT(*) as mentions\n",
    "    FROM entities\n",
    "    WHERE entity_type = 'MONEY'\n",
    "    GROUP BY entity_text\n",
    "    ORDER BY mentions DESC\n",
    "    LIMIT 30\n",
    "''', conn)\n",
    "\n",
    "print(\"Most Frequently Mentioned Dollar Amounts:\")\n",
    "df_money.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse and analyze money amounts\n",
    "import re\n",
    "\n",
    "def parse_money(text):\n",
    "    \"\"\"Parse money string to numeric value.\"\"\"\n",
    "    text = text.replace(',', '').replace('$', '')\n",
    "    multiplier = 1\n",
    "    if 'billion' in text.lower() or text.endswith('B'):\n",
    "        multiplier = 1e9\n",
    "    elif 'million' in text.lower() or text.endswith('M'):\n",
    "        multiplier = 1e6\n",
    "    \n",
    "    # Extract numeric value\n",
    "    match = re.search(r'[\\d.]+', text)\n",
    "    if match:\n",
    "        return float(match.group()) * multiplier\n",
    "    return None\n",
    "\n",
    "# Apply parsing\n",
    "df_money['value'] = df_money['entity_text'].apply(parse_money)\n",
    "df_money_valid = df_money[df_money['value'].notna()].copy()\n",
    "\n",
    "print(f\"\\nParsed {len(df_money_valid)} valid money amounts\")\n",
    "print(f\"\\nLargest amounts mentioned:\")\n",
    "df_money_valid.nlargest(10, 'value')[['entity_text', 'value', 'mentions']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Damage Metrics\n",
    "\n",
    "Analyze damage-related metrics extracted from reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Damage metrics\n",
    "df_damage = pd.read_sql_query('''\n",
    "    SELECT entity_text, COUNT(*) as mentions\n",
    "    FROM entities\n",
    "    WHERE entity_type = 'DAMAGE_METRIC'\n",
    "    GROUP BY entity_text\n",
    "    ORDER BY mentions DESC\n",
    "    LIMIT 30\n",
    "''', conn)\n",
    "\n",
    "print(\"Damage Metrics Extracted:\")\n",
    "df_damage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rainfall amounts\n",
    "df_rain = pd.read_sql_query('''\n",
    "    SELECT entity_text, COUNT(*) as mentions\n",
    "    FROM entities\n",
    "    WHERE entity_type = 'RAINFALL'\n",
    "    GROUP BY entity_text\n",
    "    ORDER BY mentions DESC\n",
    "    LIMIT 20\n",
    "''', conn)\n",
    "\n",
    "print(\"Rainfall Amounts Mentioned:\")\n",
    "df_rain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wind speeds\n",
    "df_wind = pd.read_sql_query('''\n",
    "    SELECT entity_text, COUNT(*) as mentions\n",
    "    FROM entities\n",
    "    WHERE entity_type = 'WIND_SPEED'\n",
    "    GROUP BY entity_text\n",
    "    ORDER BY mentions DESC\n",
    "    LIMIT 20\n",
    "''', conn)\n",
    "\n",
    "print(\"Wind Speeds Mentioned:\")\n",
    "df_wind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Entity Co-occurrence Analysis\n",
    "\n",
    "Analyze which entities appear together in documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find documents mentioning specific disasters\n",
    "def get_docs_for_entity(entity_text):\n",
    "    \"\"\"Get document IDs mentioning a specific entity.\"\"\"\n",
    "    df = pd.read_sql_query('''\n",
    "        SELECT DISTINCT document_id\n",
    "        FROM entities\n",
    "        WHERE entity_text LIKE ?\n",
    "    ''', conn, params=(f'%{entity_text}%',))\n",
    "    return set(df['document_id'])\n",
    "\n",
    "# Find entities that co-occur with Harvey\n",
    "harvey_docs = get_docs_for_entity('Harvey')\n",
    "print(f\"Documents mentioning Harvey: {len(harvey_docs)}\")\n",
    "\n",
    "# Find top counties in Harvey documents\n",
    "df_harvey_counties = pd.read_sql_query(f'''\n",
    "    SELECT entity_text, COUNT(*) as mentions\n",
    "    FROM entities\n",
    "    WHERE entity_type = 'TX_COUNTY' \n",
    "    AND document_id IN ({','.join(map(str, harvey_docs))})\n",
    "    GROUP BY entity_text\n",
    "    ORDER BY mentions DESC\n",
    "    LIMIT 15\n",
    "''', conn) if harvey_docs else pd.DataFrame()\n",
    "\n",
    "if len(df_harvey_counties) > 0:\n",
    "    print(\"\\nTop Counties in Harvey Documents:\")\n",
    "    print(df_harvey_counties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Entities by Document Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entities by document category\n",
    "df_by_category = pd.read_sql_query('''\n",
    "    SELECT d.category, e.entity_type, COUNT(*) as count\n",
    "    FROM entities e\n",
    "    JOIN documents d ON e.document_id = d.id\n",
    "    GROUP BY d.category, e.entity_type\n",
    "    ORDER BY d.category, count DESC\n",
    "''', conn)\n",
    "\n",
    "# Pivot for heatmap\n",
    "df_pivot = df_by_category.pivot_table(\n",
    "    index='category', \n",
    "    columns='entity_type', \n",
    "    values='count', \n",
    "    fill_value=0\n",
    ")\n",
    "\n",
    "print(\"Entity counts by document category:\")\n",
    "df_pivot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap of entity distribution\n",
    "if len(df_pivot) > 0:\n",
    "    # Select top entity types\n",
    "    top_types = df_types.head(10)['entity_type'].tolist()\n",
    "    df_heatmap = df_pivot[top_types] if all(t in df_pivot.columns for t in top_types) else df_pivot.iloc[:, :10]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    sns.heatmap(df_heatmap, annot=True, fmt='d', cmap='YlOrRd', ax=ax)\n",
    "    ax.set_title('Entity Distribution by Document Category')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Export Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export all entities to CSV\n",
    "df_all_entities = pd.read_sql_query('''\n",
    "    SELECT \n",
    "        e.entity_type,\n",
    "        e.entity_text,\n",
    "        d.filename,\n",
    "        d.category,\n",
    "        d.year,\n",
    "        d.quarter,\n",
    "        e.page_number\n",
    "    FROM entities e\n",
    "    JOIN documents d ON e.document_id = d.id\n",
    "    ORDER BY e.entity_type, e.entity_text\n",
    "''', conn)\n",
    "\n",
    "output_path = EXPORTS_DIR / 'entities.csv'\n",
    "df_all_entities.to_csv(output_path, index=False)\n",
    "print(f\"Exported {len(df_all_entities):,} entities to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export summary stats\n",
    "summary_path = EXPORTS_DIR / 'entity_summary.csv'\n",
    "df_types.to_csv(summary_path, index=False)\n",
    "print(f\"Exported summary to {summary_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()\n",
    "print(\"\\nAnalysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "After entity analysis:\n",
    "\n",
    "1. **Phase 3**: Analyze financial tables (see `03_financial_analysis.ipynb`)\n",
    "2. **Phase 4**: Build semantic search with embeddings\n",
    "3. **Phase 5**: Create interactive dashboard"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
